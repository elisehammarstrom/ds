{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# ReExam 24th of August 2022, 8.00-13.00 for the course 1MS041 (Introduction to Data Science / Introduktion till dataanalys)\n",
    "\n",
    "## Instructions:\n",
    "1. Complete the problems by following instructions.\n",
    "2. When done, submit this file with your solutions saved, following the instruction sheet.\n",
    "\n",
    "This exam has 5 problems each worth 8 points for a total of 40 points, to pass you need\n",
    "15 points.\n",
    "\n",
    "## Some general hints and information:\n",
    "* Some problems are similar to the exam in January but changed.\n",
    "* Try to answer all questions even if you are uncertain.\n",
    "* Comment your code, so that if you get the wrong answer I can understand how you thought\n",
    "this can give you some points even though the code does not run.\n",
    "* Follow the instruction sheet rigorously.\n",
    "* This exam has no anonymous exam ID due to a technical issue, however this does not mean\n",
    "that the exam is not anonymous. The grading system will automatically download all the exams from Studium\n",
    "and it is at this stage that they are anonymized by a randomized ID.\n",
    "* If there are any questions, please ask the exam guards, they will escalate it to me if necessary.\n",
    "* I (Benny) will visit the exam room at around 10:30 to see if there are any questions.\n",
    "\n",
    "## Finally some rules:\n",
    "* You may not communicate with others during the exam, for example:\n",
    "    * You cannot ask for help in Stack-Overflow or other such help forums during the Exam.\n",
    "    * You may not use encrypted communications\n",
    "    * Your on-line and off-line activity is being monitored according to the examination rules.\n",
    "\n",
    "## Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "8"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 1\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "8"
   },
   "source": [
    "\n",
    "## Probability warmup\n",
    "Let's say we have an exam question which consists of $50$ yes/no questions. \n",
    "From past performance of similar students, a randomly chosen student will know the correct answer to $N \\sim \\text{binom}(50,0.8)$ questions. Furthermore, we assume that the student will guess the answer with equal probability to each question they don't know the answer to, i.e. given $N$ we define $Z \\sim \\text{binom}(50-N,1/2)$ as the number of correctly guessed answers. Define $Y = N + Z$, i.e., $Y$ represents the number of total correct answers.\n",
    "\n",
    "We are interested in setting a deterministic threshold $T$, i.e., we would pass a student at threshold $T$ if $Y \\geq T$. Here $T \\in \\{0,1,2,\\ldots,50\\}$.\n",
    "\n",
    "1. [3p] Produce a simulation of $1000$ students. Hint: Simulate $N$ first then simulate $Y \\mid N$ and add the results. Numpy has `numpy.random.binomial` which you can simulate from.\n",
    "2. [3p] For each threshold $T$, produce a simulation as above and estimate the probability that the student *knows* less than $40$ correct answers given that the student passed, i.e., $N < 40$. Put the answer in `problem11_probabilities` as a list.\n",
    "3. [2p] What is the smallest value of $T$ such that if $Y \\geq T$ then we are 90\\% certain that $N \\geq 40$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1: [41. 47. 46. 47. 48. 44. 43. 44. 45. 47. 44. 43. 47. 47. 47. 46. 45. 41.\n",
      " 46. 47. 44. 46. 41. 41. 43. 45. 46. 43. 41. 43. 44. 46. 47. 47. 45. 46.\n",
      " 44. 46. 45. 45. 46. 41. 41. 45. 47. 46. 46. 43. 46. 41. 40. 49. 46. 43.\n",
      " 45. 47. 48. 48. 47. 44. 46. 43. 44. 43. 44. 48. 46. 45. 48. 44. 43. 47.\n",
      " 46. 43. 43. 48. 45. 42. 46. 47. 42. 41. 44. 46. 46. 44. 46. 44. 43. 42.\n",
      " 44. 47. 45. 46. 49. 47. 46. 45. 48. 41. 43. 46. 47. 47. 44. 45. 43. 46.\n",
      " 46. 43. 47. 47. 46. 45. 44. 42. 48. 46. 44. 45. 43. 42. 43. 47. 47. 48.\n",
      " 44. 48. 42. 48. 48. 46. 43. 45. 46. 41. 47. 46. 43. 47. 46. 43. 41. 47.\n",
      " 43. 46. 44. 45. 45. 47. 44. 44. 45. 47. 43. 47. 45. 47. 43. 41. 40. 49.\n",
      " 45. 49. 46. 46. 45. 45. 37. 46. 46. 41. 48. 47. 47. 48. 41. 48. 44. 47.\n",
      " 47. 47. 47. 45. 46. 46. 43. 44. 48. 46. 44. 46. 49. 44. 46. 46. 49. 46.\n",
      " 47. 48. 47. 41. 49. 45. 42. 48. 46. 45. 46. 44. 47. 45. 48. 46. 43. 45.\n",
      " 47. 45. 46. 45. 46. 46. 47. 44. 41. 46. 44. 45. 46. 43. 45. 43. 48. 46.\n",
      " 48. 48. 42. 44. 43. 45. 44. 47. 43. 48. 47. 45. 44. 46. 43. 43. 47. 42.\n",
      " 49. 43. 49. 46. 44. 43. 47. 45. 45. 43. 43. 43. 44. 45. 47. 46. 46. 42.\n",
      " 46. 47. 45. 48. 45. 44. 46. 46. 46. 45. 43. 44. 44. 47. 47. 43. 44. 44.\n",
      " 43. 46. 46. 44. 44. 46. 48. 45. 47. 48. 47. 47. 45. 44. 47. 45. 46. 42.\n",
      " 45. 46. 47. 43. 44. 42. 47. 47. 45. 43. 45. 44. 46. 42. 47. 42. 44. 41.\n",
      " 46. 45. 44. 43. 40. 49. 45. 43. 44. 44. 41. 45. 43. 47. 47. 43. 44. 47.\n",
      " 46. 47. 44. 46. 47. 46. 46. 42. 43. 46. 43. 43. 38. 46. 42. 47. 48. 46.\n",
      " 42. 41. 45. 49. 45. 43. 46. 40. 44. 49. 42. 43. 46. 43. 43. 43. 45. 43.\n",
      " 46. 48. 47. 48. 46. 44. 48. 45. 44. 46. 44. 42. 46. 46. 46. 44. 43. 48.\n",
      " 45. 43. 45. 42. 46. 42. 46. 47. 49. 37. 45. 44. 49. 46. 44. 50. 44. 48.\n",
      " 46. 43. 45. 48. 45. 46. 46. 43. 44. 44. 46. 42. 46. 46. 46. 46. 46. 43.\n",
      " 46. 43. 45. 45. 42. 46. 46. 45. 41. 45. 48. 48. 44. 45. 41. 44. 48. 44.\n",
      " 46. 47. 43. 48. 47. 47. 47. 45. 45. 43. 44. 45. 46. 44. 45. 43. 47. 46.\n",
      " 43. 48. 43. 48. 44. 46. 48. 45. 40. 46. 47. 45. 45. 44. 45. 46. 44. 46.\n",
      " 48. 50. 45. 45. 45. 46. 47. 43. 40. 49. 44. 46. 43. 43. 45. 47. 47. 44.\n",
      " 46. 46. 42. 44. 47. 47. 43. 43. 46. 45. 47. 44. 47. 44. 44. 42. 43. 45.\n",
      " 43. 45. 47. 46. 43. 46. 46. 45. 44. 45. 46. 45. 47. 42. 46. 43. 41. 44.\n",
      " 44. 44. 43. 43. 46. 45. 42. 50. 47. 43. 45. 47. 48. 45. 45. 42. 48. 47.\n",
      " 44. 46. 44. 43. 46. 47. 44. 48. 47. 46. 45. 44. 45. 47. 46. 47. 45. 48.\n",
      " 47. 46. 45. 47. 48. 49. 45. 41. 46. 43. 47. 45. 41. 43. 44. 47. 42. 44.\n",
      " 42. 45. 48. 42. 47. 47. 47. 45. 43. 42. 45. 47. 47. 46. 44. 46. 45. 43.\n",
      " 47. 43. 47. 45. 47. 43. 45. 43. 46. 45. 45. 43. 47. 47. 46. 44. 47. 47.\n",
      " 47. 47. 45. 45. 44. 41. 43. 46. 45. 43. 41. 45. 46. 46. 45. 46. 45. 45.\n",
      " 43. 46. 45. 45. 45. 42. 40. 40. 44. 44. 43. 44. 46. 44. 44. 44. 49. 45.\n",
      " 49. 48. 48. 42. 46. 47. 44. 41. 46. 44. 45. 43. 38. 44. 47. 46. 44. 46.\n",
      " 47. 38. 42. 46. 47. 47. 48. 44. 42. 44. 44. 46. 47. 47. 41. 45. 44. 43.\n",
      " 46. 47. 46. 45. 48. 45. 42. 43. 42. 43. 46. 46. 45. 44. 42. 45. 44. 44.\n",
      " 45. 42. 45. 46. 46. 45. 47. 44. 41. 43. 41. 45. 45. 47. 48. 46. 44. 47.\n",
      " 44. 42. 45. 45. 47. 47. 44. 45. 45. 46. 46. 47. 48. 44. 43. 46. 49. 43.\n",
      " 47. 45. 47. 41. 40. 44. 43. 43. 44. 47. 48. 39. 46. 46. 49. 42. 47. 42.\n",
      " 44. 47. 42. 44. 46. 45. 46. 44. 44. 46. 43. 42. 47. 47. 42. 50. 47. 44.\n",
      " 45. 49. 46. 44. 48. 45. 46. 42. 48. 50. 43. 46. 46. 46. 47. 43. 43. 42.\n",
      " 45. 45. 45. 44. 45. 45. 45. 46. 43. 48. 45. 44. 46. 49. 44. 44. 47. 45.\n",
      " 47. 46. 45. 47. 41. 43. 44. 46. 48. 43. 43. 43. 44. 46. 44. 46. 46. 47.\n",
      " 42. 49. 46. 46. 44. 45. 46. 46. 48. 46. 44. 45. 47. 47. 43. 48. 45. 45.\n",
      " 46. 44. 47. 47. 44. 46. 46. 41. 45. 43. 45. 44. 42. 43. 46. 47. 44. 44.\n",
      " 43. 47. 45. 49. 41. 45. 45. 47. 48. 44. 45. 47. 49. 48. 45. 46. 44. 47.\n",
      " 47. 49. 44. 47. 48. 47. 44. 46. 47. 45. 47. 43. 46. 41. 44. 45. 47. 47.\n",
      " 46. 46. 46. 48. 43. 46. 42. 42. 45. 46. 43. 45. 44. 45. 44. 43. 44. 45.\n",
      " 44. 45. 45. 45. 40. 43. 45. 43. 45. 42. 41. 48. 48. 46. 50. 44. 39. 41.\n",
      " 44. 45. 44. 40. 44. 46. 43. 48. 42. 40. 40. 46. 46. 46. 41. 43. 43. 45.\n",
      " 40. 45. 44. 43. 47. 43. 46. 45. 45. 45. 45. 44. 44. 41. 45. 47. 45. 45.\n",
      " 46. 47. 40. 48. 47. 44. 45. 46. 45. 44.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N = np.random.binomial(50, 0.8, 1000)\n",
    "Z = lambda n: np.random.binomial(50-n, 0.5)\n",
    "\n",
    "results = np.array([])\n",
    "for n in N:\n",
    "    results = np.append(results, n + Z(n))\n",
    "\n",
    "# Part 1:\n",
    "problem1_1000_samples = results\n",
    "print(f\"Problem 1: {problem1_1000_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 2: [0.398      0.436      0.4        0.423      0.419      0.407\n",
      " 0.412      0.402      0.402      0.418      0.418      0.403\n",
      " 0.403      0.426      0.436      0.413      0.408      0.401\n",
      " 0.421      0.391      0.416      0.441      0.437      0.424\n",
      " 0.43       0.431      0.392      0.427      0.433      0.428\n",
      " 0.428      0.44       0.421      0.417      0.401      0.416\n",
      " 0.384      0.3993994  0.433      0.41023069 0.3836858  0.39384615\n",
      " 0.37048832 0.34364261 0.30357143 0.23763955 0.16091954 0.12867647\n",
      " 0.06315789 0.02380952 0.2       ], shape: (51,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 2: \n",
    "# replace XXX to represent P(N < 40) for T = [0,1,2,...,50], i.e. your answer should be a list\n",
    "# of length 51.\n",
    "results_t = np.array([])\n",
    "for t in range(51):\n",
    "    N = np.random.binomial(50, 0.8, 1000)\n",
    "    Z = lambda n: np.random.binomial(50-n, 0.5)\n",
    "    res = np.array([[n, n+Z(n)] for n in N])\n",
    "    #print(res.shape)\n",
    "    res_passed_and_n_40  = res[(res[:, 0] < 40) & (res[:, 1] >= t)]\n",
    "    res_passed = res[res[:, 1] >= t]\n",
    "    #print(res_passed_and_n_40.shape, res_passed.shape)\n",
    "    results_t = np.append(results_t, res_passed_and_n_40.shape[0]/res_passed.shape[0])\n",
    "problem1_probabilities = results_t\n",
    "print(f\"Problem 2: {problem1_probabilities}, shape: {problem1_probabilities.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 3: t: 48, r: 0.06315789473684211\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 3: Give an integer between 0 and 50 which is the answer to 2.\n",
    "for t, r in enumerate(problem1_probabilities):\n",
    "    if 1-r >= 0.90:\n",
    "        print(f\"Problem 3: t: {t}, r: {r}\")\n",
    "        break\n",
    "problem1_T = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "8"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 2\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "8"
   },
   "source": [
    "\n",
    "In many areas of data science and machine learning we need to produce random samples in different ways. This can be done to compute difficult integrals or validate algorithms. \n",
    "\n",
    "1. [2p] Produce 1000 samples from the distribution below using inversion sampling\n",
    "$$\n",
    "    F[x] = \n",
    "    \\begin{cases}\n",
    "        0, & x \\leq 0 \\\\\n",
    "        \\sin(x), & 0 < x < \\pi/2 \\\\\n",
    "        1, & x \\geq \\pi/2\n",
    "    \\end{cases}\n",
    "$$\n",
    "and show your result with a histogram \"You can use sagemath function `histogram`, or `matplotlib.pyplot` `hist`\". Also what is the true density? Provide a plot of the true density between 0 and $\\pi/2$.\n",
    "\n",
    "2. [3p] Consider a random variable $X \\sim F$ sampled from distribution $F$. Your goal is to estimate $E[X]$. Do this by producing 1000 different experiments, each sampling 1000 samples from $X$ and compute the empirical mean. Provide the $0.025$ and the $0.975$ quantile of the experiments.\n",
    "3. [3p] Use Hoeffdings inequality to produce a 95% confidence interval for the estimated mean above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 4: [0.12219062694577947, 0.2551526772791508, 1.1807803382233715, 0.23294833901473924, 0.9275627654029033, 0.1684930101000768, 0.02796219063312067, 0.9181022292771375, 1.3055910490282638, 0.028624336714183556, 0.3563485458894871, 0.6253656378422584, 1.2528901450920298, 0.40674095477312183, 1.0646583170848505, 1.3619726029920272, 0.5061002026509624, 0.2503734987588191, 1.404183010539606, 0.40317100613945256, 1.0003589002934372, 1.273285143955823, 0.43420674263821385, 1.297266749924887, 0.13413099667676623, 1.1303596306549692, 0.8976704466303744, 1.056188193371444, 1.478589596081079, 0.9212693908445184, 0.18258265094191373, 0.47212444915758733, 1.374582467479122, 0.200807022440817, 0.058014666262485935, 1.4305128784453667, 0.9501329141931337, 0.4488269447548138, 1.1755318490536675, 1.1709224661514221, 0.5908708427996882, 1.0515357399669252, 1.030523924376612, 0.7395335226400843, 0.3817893319713448, 0.5809130928139308, 1.0655801960108005, 1.0413038748270644, 0.7638452211477071, 1.292426844849797, 0.10077628649250207, 0.5036996415775763, 0.7072406655965349, 0.011699831244474766, 0.2240038691813604, 0.3316616290705623, 0.1304810782202331, 0.18045746237635538, 1.0105086379145305, 0.7484622778906008, 0.9515076726098591, 0.5597810296047636, 0.7351856854012749, 0.09216577750710483, 1.2019228882149982, 0.7421532597393274, 0.5198043094131535, 0.8220809450967416, 0.3944250413129678, 0.2156145878025177, 0.9948049807649201, 0.016655281357172817, 0.16199053655164297, 0.22562600709853367, 0.3550432900986196, 0.09475869691108714, 0.5419360563540102, 0.15486268349807739, 1.393948073403713, 0.3185278236736233, 0.6732689150664166, 0.3321719694435366, 0.551582914561207, 1.2676045245132428, 0.03673476909382093, 0.8302985662858984, 0.6047519068620605, 0.07196804600600169, 0.3809890781918362, 0.23448138838050997, 0.6415690384602275, 0.3264165793780419, 0.4359734370245258, 0.6231859921803794, 0.8999792061067543, 0.036501085758664, 0.7999787768212493, 0.5524316995898108, 0.8158361559635124, 0.831186738469187, 0.8398943931177816, 0.06171385274707688, 1.334159891847174, 0.6377295878377506, 1.158992897218202, 0.11843064349799853, 1.2230937161114037, 0.8345723829663431, 1.0666692624402636, 0.5891148817737554, 0.9343499630261691, 1.0380862264304456, 1.0085525376327273, 1.1961921060765603, 0.40293585068261045, 0.3743437847958389, 0.24480542593378435, 0.35973872365747733, 0.6026165104451199, 0.08904213508487065, 0.9520890375751839, 0.6791043620312465, 0.5310016040769442, 0.8682401669674668, 0.41896006132256786, 0.9960488880003793, 0.16818531373225573, 0.17087863460464006, 0.41724041556057306, 0.3781841656856441, 0.5921974294870435, 0.6548574879567046, 1.3253754292633406, 0.14206850184619252, 0.046118944624006855, 0.3955873119965732, 0.09906133030800666, 0.7320226587677374, 1.3744269001923235, 0.5867728654799598, 0.2869579000345173, 0.01767335105749554, 0.6513276669387995, 0.1966681532819467, 0.9277107606342581, 0.06976185792305245, 0.8412424180929922, 0.7371959322149781, 1.2391019432403296, 0.41085777792435485, 0.13271673327256642, 0.13925437707996038, 0.15190302738145073, 0.35648478181844107, 0.28336774028259815, 0.5061003364460672, 0.1617242049452822, 0.07255124345802981, 0.24815621321549103, 0.9101995307580889, 0.26566073025008263, 1.4657904406440587, 0.7124972563200757, 0.4506059086450328, 0.2890352447930493, 0.6706118832557865, 0.6332168068902513, 0.3681662604400014, 0.3181824721304275, 0.5445950584859326, 0.9746708061364888, 0.5726840270174957, 0.9541109733656792, 0.8075850823721824, 0.27951372619379944, 0.5132434406615323, 1.0059496012152365, 0.7960438129168259, 1.2077427255947712, 1.3446862202479057, 0.6931164664679291, 0.30542716377378526, 0.3483374189988802, 0.11434474740784836, 0.16303359631627448, 1.127691818716015, 0.6328838389801821, 0.3664218462041665, 0.3042454989244336, 1.363679845654834, 0.14111729749600327, 0.29843615572617743, 0.11506030787196204, 0.7869350488669588, 0.32198398880813267, 0.6806983400480883, 1.2855854367038642, 0.25831466846334494, 0.022139398778308, 0.23621685275454313, 0.6572409582886898, 1.0517120441071637, 0.07100193238904996, 0.2062851967619794, 0.5056250342591894, 0.6016687466882557, 0.4200013703694935, 0.19351740169021173, 0.582723287420303, 1.3180799920426978, 0.410567317024212, 1.4376217204503285, 0.2728824060695382, 0.8043692351853533, 0.30644783551265564, 0.08878404330826459, 0.0055489571076247045, 0.02253342923059842, 0.36758380871544094, 0.880328399569924, 1.1343432110764007, 0.3895948816135283, 0.04995667055742396, 0.7469559089456868, 1.0301176493301347, 0.4671470479658719, 1.1506979610852313, 0.6887513396197451, 0.4840348968521406, 0.03544252344024334, 0.9962103093358412, 0.6686680147615035, 0.8490966147711192, 1.1689207835686355, 0.0685707953468491, 0.7551309369694517, 0.6345641003344535, 0.488723118857661, 0.6650349815255658, 0.35062419233311976, 0.6005632518638401, 0.8469127439630535, 0.07528365946948401, 1.2932776498079896, 0.6755125046771354, 0.9000845023705338, 0.40446562442479794, 0.26262408390020564, 0.125304108696041, 0.9980287490719951, 0.6062294874495272, 0.3663695460893456, 1.0445309964641034, 0.5896328398062154, 0.14824365875556178, 1.423869297414125, 0.38666529911645964, 0.39846748447364927, 0.22042080475969833, 0.16478842072429065, 0.0940316892412113, 0.14984143731321847, 0.7359491983438273, 1.152609374216488, 0.021878857246848085, 0.41018144831063563, 0.1595557449717755, 1.4252047292918297, 0.0343313057800512, 1.271554899534375, 0.39090831953932076, 1.2306758364783104, 0.29324783163283485, 0.1599082888989215, 0.2350206266591228, 0.3702371869275224, 1.2754861559388668, 0.8374537501368166, 0.6375887158863366, 0.27007233280050963, 0.12704631583133275, 0.6071531739726782, 0.4210422513595814, 0.22648975084896353, 1.036569209259246, 0.060234222721209346, 0.03182343333331053, 0.3630811486939881, 0.5826849838806875, 0.9075294172682163, 0.8781946970292073, 0.5484963618061198, 0.8353550467646342, 0.8797682454327088, 0.5758753150942776, 0.2930515050227686, 0.30716166090520247, 0.36327530593060353, 0.5885675984652929, 1.1048917446847941, 1.2923453241216722, 0.04055991141090053, 0.8033616766953816, 0.19012338568255613, 1.5172665585265765, 1.244207342723816, 0.0668900477102727, 0.49097170961564013, 1.1181097969502842, 0.7715343364348771, 0.12798918798756323, 0.7342428231659234, 0.17937112611990713, 0.5444660579887122, 0.8407424878740163, 0.13712630717924554, 0.39169183917990735, 0.3161012757667662, 0.8792360358009081, 0.24614627286182472, 0.07278648903056538, 0.6951245945832155, 0.37999329585423447, 0.14141184045541763, 0.19142833312766572, 0.5607915619288664, 0.7835115765721494, 0.6613407335466561, 0.30748234808092534, 1.2569823158489808, 0.43491222659119594, 1.0250829479102708, 0.12048287980766698, 0.22978901711623412, 0.3204066741926656, 0.3941122389564859, 0.493882449429516, 0.7772792666639292, 0.21585138489729053, 0.5924060614230738, 0.341683020747663, 0.845582994460956, 0.08843162050524571, 1.0124593886620794, 0.5002617208645375, 1.0901024829693793, 0.11950030993019553, 0.6770775225468885, 0.4283679565101733, 0.4740522223887248, 0.08465235110529892, 0.9290309245961604, 0.23726517466646124, 1.237500361943201, 0.07808606066410209, 1.1930381390329565, 0.09783656171491428, 0.22424924848667208, 0.4723375234852681, 0.8973593437398498, 0.6672291919776756, 0.21897706280310114, 0.6146929673872553, 0.48038298371957117, 0.3999105522469119, 0.34986602556103896, 0.6292720654157539, 1.0759318174493682, 0.32346383410606955, 0.2953586815706938, 0.25263756857221986, 0.9855602756198896, 0.2207587213252383, 1.2372567651555282, 0.6537484522546302, 1.107503919329769, 1.123745380879951, 0.8641398972764576, 1.022517424615136, 1.012383857528323, 0.5858837899932908, 0.8859997992039268, 0.2158719046188453, 0.4449971980291595, 1.2117596861324118, 0.2931329483178946, 1.304563775482739, 0.8087269086149332, 0.2862002985149859, 1.1457966246455953, 1.2101200242893007, 0.530851324092764, 0.14655069856735517, 0.36446391828460484, 0.4930926885189277, 0.4553061936573627, 0.09356501174523549, 0.2207423614262764, 0.09094731194005638, 0.15696950087334924, 1.2663585215319795, 0.33515401692785096, 0.5784641423464736, 0.9419895658853994, 1.345569408611984, 0.310399966180955, 1.1920656680163122, 0.5247151720242258, 0.7861484705694302, 1.2342656249752455, 0.5777862256634084, 0.14171588297639126, 0.5903770455621847, 0.1336013530919856, 1.1027985844791508, 0.5402130471255823, 0.661331236999235, 0.6939233963791079, 0.179662365335335, 0.3440554874011556, 0.6098289544741293, 0.34058258765057114, 0.20390264043530512, 0.19127327486582962, 1.1545640491326272, 0.9258627007701293, 0.9675579977840282, 0.816719232036302, 0.24883432751291856, 1.40237102257193, 0.847192218068402, 0.25098975559987385, 0.3822683876051377, 0.9327364978588447, 0.16230763806895646, 0.032286469882242826, 0.6781067816700832, 1.0616629325542413, 0.726251411371943, 1.1336451296522947, 0.042919373049418306, 0.0017127860563910628, 0.7663558223826332, 0.7798736399111021, 1.4123523008695107, 0.4191658191328317, 0.2829691779046202, 1.2453589269775505, 0.8323134125872326, 1.366326917166185, 1.4137420651232084, 1.085479775995498, 1.0152020736984695, 0.06703471259615164, 0.5281474540566874, 0.2969846625661282, 0.9946194036518735, 0.9749984572912799, 0.6798280641105359, 0.3975133249769531, 0.9314875638631746, 1.266100201011649, 0.008116795050958307, 0.12568679124855678, 0.8348320789989994, 1.243355199302472, 0.4877108536228728, 0.6059444105675174, 1.30086922027538, 0.5408555699812575, 0.1821678752320739, 0.6091066513681251, 0.2318580053481614, 0.2548810345526553, 0.03353607581650239, 0.35312052026612595, 0.14720613681219824, 0.49867966359864674, 0.12958662523335612, 0.6307259872198323, 0.42604940630989724, 0.8737168888372149, 0.34238557545321197, 0.32563088493560666, 0.7454778059827938, 0.12784157213962946, 0.6767504100845552, 0.03272438007185977, 0.4968925280569868, 0.5162989214260887, 0.1411575773543973, 1.2301163222982843, 0.1177681203408482, 0.0435100288804578, 0.36632237085044644, 1.076206612469438, 1.067316589223335, 0.24754161432471844, 0.3010661784000607, 0.9944362047426581, 0.643222392572648, 0.20804296573197373, 0.5660067243835472, 0.3175250157404538, 0.6590813526929254, 0.28086857391928266, 0.05587956564181165, 0.04282281938040464, 0.3062876049590721, 1.2672699213091656, 0.37345901304536994, 1.0340346177352584, 0.0013900630351552134, 0.23643754546109338, 0.1508671712093228, 0.05169698247752341, 0.39056173775503505, 0.2553624103052242, 0.44170590464226483, 0.04398832852448095, 0.4000961201932739, 1.112154110891142, 0.24417930435517515, 0.005085191224348132, 0.4675538205960534, 0.8833411394026457, 0.3384186942065732, 0.6775481651107831, 0.03917964700655917, 1.2401938298577808, 0.2013943208805972, 1.278531289409029, 0.5671575466623788, 1.04176455673065, 0.7871029196311553, 1.2693850887212232, 0.8389975009905025, 0.34317570985353335, 0.6538347111821755, 0.3092942039705095, 0.2552092625875162, 0.2625329387185513, 0.337655431220403, 1.1409848092581758, 1.1453188120876314, 0.13323751566895053, 0.38532860606419317, 0.5608692944705563, 0.06808017230918335, 1.449014827988469, 1.043301263114395, 0.5685925303811004, 1.2292961467071195, 0.18925068790360652, 0.3545716912166755, 0.24296241207836825, 0.788635715158544, 0.0730145074202254, 0.6954575241553492, 0.8936280936635543, 0.27043061336542307, 0.1634707294268394, 0.517379904120618, 0.8981021876156543, 1.1334958279858445, 0.24767241953951613, 0.9126287056368015, 0.1330252841444391, 0.5193075478236868, 0.2598496194263412, 0.9805084124246869, 0.5184022935445678, 0.17003538079025918, 0.12415220633990748, 0.3857438554908207, 1.2263761394587027, 0.3023162320559186, 0.5379731389650657, 0.3205869337886949, 0.6083724071330705, 0.8485407423999967, 0.4823271471290286, 0.6143935455698128, 0.8009915652215078, 0.044195680914611006, 0.2976546680479521, 0.4642797510132094, 1.2002919106568575, 0.3676464172783296, 0.15176885558188505, 0.38555546913733785, 0.967808817353402, 1.103846183917963, 0.6566427087550181, 0.7053297180830733, 0.28304917385783157, 0.6020532533166525, 0.12456027696608706, 0.33290160938574775, 0.2227390155124355, 1.1194579472779345, 0.4705678682038641, 0.7068302526756133, 0.26611721678995454, 0.07008748384520212, 0.7210329269255922, 0.058268780152584077, 1.4903725473255314, 0.8036445607236479, 0.07073544828986127, 0.5067000942772533, 0.6538155546135551, 0.010327474080054843, 1.1098834586862483, 0.4707194336894118, 0.04410197792700757, 0.1413042048098467, 0.5202904451972796, 0.37656283617674147, 1.0241677101250872, 0.25801813698320314, 0.4180802219132508, 0.06621923114112407, 0.1902295827341897, 0.5700486089381419, 0.045048165193641444, 0.6999042143305176, 0.458205478012549, 0.8920606927478304, 0.12741410982796936, 0.801024991823035, 0.23335240287706713, 1.038682565719471, 0.19830854935544248, 0.6964862037151659, 0.199545449441861, 1.3634901599049578, 0.31471534465992707, 1.0045758307293724, 0.28464689541124444, 0.27143343732973185, 0.10605035687928566, 0.5386477595450649, 0.79795862087353, 0.6018284001766011, 0.5995151573090861, 0.6738736117853835, 0.6748788402453829, 0.43077323810513407, 1.0772862062691066, 0.25723572388816873, 0.14972447317283105, 0.3621174112950053, 0.8203408388937612, 1.2792105305482058, 0.4600045209368464, 1.0319712417130713, 0.6563575118326448, 0.95562554149043, 1.28868212140658, 0.35284038734891693, 0.9086993382933767, 1.091646465971649, 0.7685699083869578, 0.2743216016110083, 0.07618901716809087, 0.18023927106853707, 0.6134708922942701, 0.8658798269108362, 0.4335084989723635, 0.9020254172587917, 0.6613843125846677, 0.025196771703634557, 0.07608350304540404, 0.9113039250686388, 0.4764893186787176, 1.0351419904121877, 0.1915488742812171, 1.1810792581437595, 1.3922065069687672, 0.434594249763892, 1.1021994652670288, 0.29151124315905613, 0.05937464750197306, 0.04294726474915152, 1.0721070901944856, 0.03178342210937403, 1.4391360741538908, 0.017720060659368948, 0.8407000865408959, 1.057344498627002, 0.09553631393863964, 0.6115548327253176, 0.6056966801642364, 0.9752859989154962, 0.8692596855421908, 0.746046371578681, 0.5497133185941602, 0.05426667090092244, 0.06806868051390103, 0.7850823713935989, 0.5507099151218212, 1.0072516385477863, 0.687162220343875, 0.7699824169407059, 0.8140863135917508, 0.12263949065898831, 0.3766246060326364, 1.0373329382913965, 1.0993393526790847, 0.847222016930561, 0.7309313983600463, 0.5977335249112623, 0.30003147934026464, 0.22625948751331962, 0.3631879790164848, 0.655510871507082, 1.0543975938762988, 1.4810584805860363, 1.1535664467718483, 0.2281936115913893, 0.6036513861395199, 1.1351239724828759, 0.889234563230117, 0.06149483333420276, 1.3170612570741886, 1.09809782490017, 0.9359267412305011, 0.6587899957350312, 1.3955450947343988, 0.2425491209293372, 0.9484136586656797, 0.2531359473673433, 0.2850212348662374, 0.18924112549566227, 0.33817618955937856, 0.4576562135055146, 0.44767870971183066, 0.9645502968972386, 0.7818619680106322, 0.6273391822021585, 0.8337788246401925, 1.033765061694007, 0.6908344883533029, 0.6710707863635282, 0.909623411444917, 1.1025538239838821, 0.12534103880689354, 0.42681773277412355, 0.6852100961044083, 0.5800219294066906, 1.4023658770716583, 0.5939655362888263, 0.5285496774951652, 0.0552494645921683, 0.6010130140769077, 1.1871467220832457, 0.45085880860968724, 0.3062621887309082, 0.8177079685082742, 0.972463359763452, 0.45450167925429036, 0.9515532744390458, 0.33335562376202277, 0.46602475285153155, 0.47466035314360694, 0.9244073821163014, 0.31993164713579963, 0.3186623763597501, 0.7021081490084308, 0.27997419430055515, 0.05355269938436212, 0.35212191698760015, 1.3470371097995217, 0.00851513071324071, 0.7123055220231445, 1.0861877796732857, 1.1589995396145545, 0.5318426646287341, 0.041314374318322376, 0.39355211062387896, 0.9854769411857397, 0.13665752181791585, 1.3134837340678989, 0.6567920706697516, 1.0835297519019664, 0.17584677444043742, 0.8737922750942486, 0.5090329533508973, 0.4830596943023368, 0.4825312122515568, 0.24803810133086224, 0.40709598419330734, 0.8071130162685098, 0.6597874036435899, 0.24775856983933556, 1.0137581822042072, 0.5651479735705732, 0.7112900380694523, 0.14572596111949737, 0.3121888853092864, 0.3891721763033388, 0.2860424512742927, 0.16445100533966892, 0.8347329337633755, 0.8464641443097177, 0.45634496526774415, 1.1528957937140214, 0.2930995075281422, 0.30148692683383604, 0.34705504496776385, 0.7394217133986758, 0.6115275162724846, 0.7355265864781134, 0.3446527699263865, 1.4422406699387427, 0.18920975093453532, 0.09159322180856182, 0.5406935751332296, 0.36546890045052266, 0.7455376486564506, 0.12144100717958833, 0.8110067235933913, 0.7130193361637064, 0.5666346046730754, 0.1258320490491553, 1.218356148740949, 0.5084350752677788, 0.2206566346962966, 0.5718726614644128, 0.2602674278264031, 1.2137712245818049, 0.42529471794290613, 1.0843556661998435, 0.5920327685462956, 0.7693692861544079, 0.10560257602241337, 1.030235270311083, 0.5674837140268426, 0.3523577596859948, 0.11188253744774837, 0.6419854742110114, 0.5736432643442605, 1.1499417546538935, 0.0035672979404471204, 0.9141927882249705, 0.37185133199968284, 1.0721584007871912, 1.1400575229113739, 0.2669925991890295, 0.10294828713630248, 1.282045008588518, 1.2741743973658288, 0.3543795513764033, 0.9609311239350831, 0.10461984021127152, 0.9589855115238212, 0.14677143822529753, 1.4636600215659588, 0.9828979010248583, 0.6127493648087535, 0.4432710709656416, 0.8466460530204022, 1.3839759270526402, 0.7850325758704022, 0.9986580596685671, 0.8085725483357353, 0.39706344897879386, 0.3156151855898049, 0.7274937351800557, 0.2511272522444836, 1.079935622796792, 0.41843310916020304, 1.4574428468457719, 0.9717835200490323, 0.43175189435303335, 0.657344248947885, 0.4585745961997824, 0.1176587057273477, 0.13266777104744354, 0.6695360867768704, 0.4140819501993408, 0.8590860296815931, 1.4164155050403162, 1.0526291136618346, 0.1520605525543814, 1.3540746706192734, 1.1121198677281137, 0.23685295804879591, 0.6651832434015398, 0.09195324737462807, 0.4702552610198143, 0.4231550481537665, 0.6171485526654669, 0.9093319526170404, 0.03335352657349869, 0.3014173622739729, 0.5910783260757309, 0.5896443871696715, 0.9332794455762297, 1.2769546991136476, 0.3799865852743415, 0.6451617538838397, 0.16082538105401045, 0.8327445945136891, 0.518858270845867, 0.8572768830067278, 0.7383572479052328, 0.9837968129155941, 0.27974628830666065, 0.2438663519135248, 0.6248564406556222, 0.4624372302189789, 1.06461765647529, 0.9000268521262541, 0.032922518773116576, 0.8505876285236382, 0.46263539934724957, 0.6426958413996261, 0.7852134697343712, 0.00475776431905621, 0.9934195218625351, 0.295529156043389, 0.1608481946110706, 0.02322047845516253, 0.7698084683119875, 0.7597618881994446, 0.7267913309759677, 0.3184534972867768, 0.2486878446462497, 0.86575337272146, 0.01862518125062885, 0.7437990260094236, 0.4936474930143941, 0.749137536153287, 0.4041710933658837, 0.7535457016993473, 0.03833453843730082, 0.21515926573902092, 0.22779850627474071, 0.36406622311876496, 0.26271201187187737, 0.23018441272852883, 0.25772132943484144, 0.9385794298842167, 0.937370801005805, 0.10003439538680352, 0.2633772625862969, 0.3098384625185856, 0.054464715832782444, 0.058398688903095256, 1.1648618263765522, 1.1597562053574204, 0.05103838938260674, 0.692879783613069, 0.7484599800460867, 1.212960196672111, 0.8876316891207521, 0.20380095592612585, 0.24038748884105576, 0.21656357635236023, 0.5575128997216268, 1.1854928222184995, 0.6376599455507128, 0.3552348906016693, 0.5544636579062647, 0.5322567495400445, 0.365059673052658, 1.2621108006839867, 0.10654814215876345, 0.014271162613222145, 0.0695278596618943, 0.6064808831950218, 1.1571574651476322, 0.4179194035441204, 0.2310642732551603, 0.5507065060447156, 1.005275055919494, 0.2699668164665074, 0.004197212909553584, 0.3397440191041884, 0.19081507092822525, 0.1113764967999216, 0.08603469693007451, 0.35594939681558596, 0.03633872556474451, 0.42725573801789335, 0.7748892498086061, 0.2982431521437235, 0.34603213447624737, 0.6536207443156866, 0.9520496209824972, 0.962848347771315, 0.2128446621995794, 0.9741146343471675, 0.6087006807845068, 0.8425845854177275, 0.9135996436042618, 0.06819263635349743, 0.7004429532238236]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6a508a0910>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# put your samples in the variable samples\n",
    "samples = [np.arcsin(i) for i in np.random.uniform(0, 1, 1000)]\n",
    "print(f\"Problem 4: {samples}\")\n",
    "x = np.linspace(0, np.pi/2, 1000)\n",
    "plt.hist(samples, bins=10, density=True)\n",
    "plt.plot(x, np.cos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 5: (1000,), (1000,), (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Produce 1000 experiments, in which each experiment you draw\n",
    "# 1000 samples from F. Store the value of the empirical mean of each\n",
    "# experiment and compute the 0.025 and the 0.975 quantiles\n",
    "samples = np.array([[np.arcsin(i) for i in np.random.uniform(0, 1, 1000)] for _ in range(1000)])\n",
    "means = np.mean(samples, axis=1) # the computed empirical means, should be a list of length 1000\n",
    "quantile_0025 = np.quantile(samples, 0.025, axis=1) # the 0.025 quantile\n",
    "quantile_0975 = np.quantile(samples, 0.975, axis=1) # the 0.975 quantile\n",
    "print(f\"Problem 5: {means.shape}, {quantile_0025.shape}, {quantile_0975.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence interval around the mean is [0.53,0.61]\n"
     ]
    }
   ],
   "source": [
    "# Put your interval in the form\n",
    "mean = np.mean(means)\n",
    "epsilon = (1/np.sqrt(1000))*np.sqrt((1/2)*np.log(2/0.05))\n",
    "l_edge = mean-epsilon # The left edge of the interval\n",
    "r_edge = mean+epsilon # The right edge of the interval\n",
    "print(\"Confidence interval around the mean is [%.2f,%.2f]\" % (l_edge,r_edge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "8"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 3\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "8"
   },
   "source": [
    "\n",
    "## Concentration of measure\n",
    "\n",
    "As you recall, we said that concentration of measure was simply the phenomenon where we expect that the probability of a large deviation of some quantity becoming smaller as we observe more samples: [8/22 points per correct answer]\n",
    "\n",
    "1. Which of the following will exponentially concentrate, i.e. for some $C_1,C_2,C_3,C_4 $ \n",
    "$$\n",
    "    P(Z - \\mathbb{E}[Z] \\geq \\epsilon) \\leq C_1 e^{-C_2 n \\epsilon^2} \\wedge C_3 e^{-C_4 n (\\epsilon+1)} \\enspace .\n",
    "$$\n",
    "\n",
    "    1. The empirical mean of i.i.d. sub-Gaussian random variables?\n",
    "    2. The empirical mean of i.i.d. sub-Exponential random variables?\n",
    "    3. The empirical mean of i.i.d. bounded random variables?\n",
    "    4. The empirical variance of i.i.d. bounded random variables?\n",
    "    5. The empirical mean of i.i.d. random variables with finite variance?\n",
    "    6. The empirical variance of i.i.d. random variables with finite variance?\n",
    "    7. The empirical variance of i.i.d. sub-Gaussian random variables?\n",
    "    8. The empirical third moment of i.i.d. bounded random variables?\n",
    "    9. The empirical fourth moment of i.i.d. sub-Gaussian random variables?\n",
    "    10. The empirical mean of i.i.d. deterministic random variables?\n",
    "    11. The empirical tenth moment of i.i.d. Bernoulli random variables?\n",
    "\n",
    "2. Which of the above will concentrate in the weaker sense, that for some $C_1$\n",
    "$$\n",
    "    P(Z - \\mathbb{E}[Z] \\geq \\epsilon) \\leq \\frac{C_1}{n \\epsilon^2}?\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Answers to part 1, which of the alternatives exponentially concentrate, answer as a list\n",
    "# i.e. [1,4,5] that is example 1, 4, and 5 concentrate\n",
    "problem3_answer_1 = [XXX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Answers to part 2, which of the alternatives concentrate in the weaker sense, answer as a list\n",
    "# i.e. [1,4,5] that is example 1, 4, and 5 concentrate\n",
    "problem3_answer_2 = [XXX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "8"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 4\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "8"
   },
   "source": [
    "\n",
    "In this problem you will be working with a text file `a_sequence.txt`, found in the `data` folder. This contains a sequence of numbers that are observations of a Markov chain. The goal of this exercise is to analyze this sequence in different ways.\n",
    "\n",
    "1. [2p] Take the file `a_sequence.txt` and load it as a list of integers. Use bash or something to figure out how to parse the file.\n",
    "2. [2p] Define a Markov chain from this list of integers\n",
    "    1. What are the states?\n",
    "    2. How many states are there?\n",
    "3. [2p] Estimate the transition probability of going from state 42 to state 16?\n",
    "4. [2p] Find the transition matrix $P$ and compute the matrix power $P^{10} v$ where $v = (1,0,\\ldots,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the file a_sequence.txt and load it as a list of integers. \n",
    "# Put your result in the variable \"numbers\"\n",
    "\n",
    "numbers = XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Construct a Markov chain of this list of integers, that is. \n",
    "# EXPLAIN in text what are the states are and what \n",
    "# the transition probabilities mean.\n",
    "\n",
    "#---------Put your explanation between the lines-------------\n",
    "\n",
    "#------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "# put the number of states in the variable n_states\n",
    "\n",
    "n_states = XXX\n",
    "\n",
    "# Now fill in the states, stored as a sorted list of integers\n",
    "states = XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "# Estimate the transition probability of going from $42$ to $16$. \n",
    "# You can use the below function if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "def makeFreqDict(myDataSeq, one = int(1)):\n",
    "    '''Make a frequency mapping out of a sequence of data - list, array, str.\n",
    "\n",
    "    Param myDataList, a list of data.\n",
    "    Return a dictionary mapping each unique data value to its frequency count.'''\n",
    "\n",
    "    freqDict = {} # start with an empty dictionary\n",
    "\n",
    "    for res in myDataSeq:\n",
    "\n",
    "        if res in freqDict: # the data value already exists as a key\n",
    "                freqDict[res] = freqDict[res] + one #int(1) # add 1 to the count\n",
    "        else: # the data value does not exist as a key value\n",
    "            # add a new key-value pair for this new data value, frequency 1\n",
    "            freqDict[res] = one\n",
    "\n",
    "    return freqDict # return the dictionary created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "# Put your answer here for the transition probability\n",
    "\n",
    "transition_probability = XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "# Fill in the transition matrix P as a numpy array of \n",
    "# shape (n_states x n_states)\n",
    "# Make sure it is a transition matrix by checking the column sum\n",
    "P =XXX\n",
    "\n",
    "# If our initial vector is\n",
    "v = np.zeros(n_states)\n",
    "v[0] = 1\n",
    "\n",
    "# What is P^10 v\n",
    "steady_state_v = XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "5",
    "lx_problem_points": "8"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 5\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "5",
    "lx_problem_points": "8"
   },
   "source": [
    "\n",
    "## SMS spam filtering [8p]\n",
    "\n",
    "In the following problem we will explore SMS spam texts. The dataset is the `SMS Spam Collection Dataset` and we have provided for you a way to load the data. If you run the appropriate cell below, the result will be in the `spam_no_spam` variable. The result is a `list` of `tuples` with the first position in the tuple being the SMS text and the second being a flag `0 = not spam` and `1 = spam`.\n",
    "\n",
    "1. [3p] Let $X$ be the random variable that represents each SMS text (an entry in the list), and let $Y$ represent whether text is spam or not i.e. $Y \\in \\{0,1\\}$. Thus $\\mathbb{P}(Y = 1)$ is the probability that we get a spam. The goal is to estimate:\n",
    "$$\n",
    "    \\mathbb{P}(Y = 1 | \\text{\"free\" or \"prize\" is in } X) \\enspace .\n",
    "$$\n",
    "That is, the probability that the SMS is spam given that \"free\" or \"prize\" occurs in the SMS. (This is precision)\n",
    "Hint: it is good to remove the upper/lower case of words so that we can also find \"Free\" and \"Prize\"; this can be done with `text.lower()` if `text` a string.\n",
    "2. [3p] Estimate the probability that the word \"free\" or \"prize\" is in the text given that it is spam. (This is recall) I.e. estimate\n",
    "$$\n",
    "    \\mathbb{P}(\\text{\"free\" or \"prize\" is in } X \\mid Y = 1) \\enspace .\n",
    "$$\n",
    "3. [2p] Provide a \"90\\%\" interval of confidence around the true probability from **part 1**. I.e. use the Hoeffding inequality to obtain for your estimate $\\hat P$. Find $l > 0$ such that the following holds:\n",
    "$$\n",
    "    \\mathbb{P}(\\hat P - l \\leq \\mathbb{E}[\\hat P] \\leq \\hat P + l) \\geq 0.9 \\enspace .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "5",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run this cell to get the SMS text data\n",
    "from exam_extras import load_sms\n",
    "spam_no_spam = load_sms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "5",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# fill in the estimate for part 1 here (should be a number between 0 and 1)\n",
    "problem5_hatP = XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "5",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# fill in the estimate for hatP for the double free question in part 2 here (should be a number between 0 and 1)\n",
    "problem5_hatP2 = XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "5",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# fill in the calculated l from part 3 here\n",
    "problem5_l = XXX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "lx_assignment_number": "vB",
  "lx_course_instance": "2021",
  "lx_course_name": "Introduction to Data Science: A Comp-Math-Stat Approach",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
